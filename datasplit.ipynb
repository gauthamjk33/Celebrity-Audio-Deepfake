{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(directory):\n",
    "    audio_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            audio, sr = librosa.load(file_path, sr=None)  # Load the audio file\n",
    "            audio_data.append((filename, audio, sr))  # Store the filename, audio data, and sample rate\n",
    "    return audio_data\n",
    "\n",
    "audio_directory = r'/home/gautham/release_in_the_wild'\n",
    "audio_list = load_audio_files(audio_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_data):\n",
    "    features = []\n",
    "    for filename, audio, sr in audio_data:\n",
    "        # Extract MFCCs (Mel-frequency cepstral coefficients) as features\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "        features.append((filename, mfccs))\n",
    "    return features\n",
    "\n",
    "# Extract MFCC features from the loaded audio\n",
    "audio_features = extract_features(audio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    file               speaker      label\n",
      "0  0.wav         Alec Guinness      spoof\n",
      "1  1.wav         Alec Guinness      spoof\n",
      "2  2.wav          Barack Obama      spoof\n",
      "3  3.wav         Alec Guinness      spoof\n",
      "4  4.wav  Christopher Hitchens  bona-fide\n",
      "5  5.wav              Ayn Rand  bona-fide\n",
      "6  6.wav          Barack Obama      spoof\n",
      "7  7.wav          Donald Trump  bona-fide\n",
      "8  8.wav          Donald Trump  bona-fide\n",
      "9  9.wav         Alec Guinness  bona-fide\n"
     ]
    }
   ],
   "source": [
    "metadata_file = r'/home/gautham/release_in_the_wild/meta.csv'\n",
    "metadata_df = pd.read_csv(metadata_file)\n",
    "print(metadata_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.columns = metadata_df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(metadata_df, pd.DataFrame(audio_list, columns=['Filename', 'Audio', 'SampleRate']), left_on='file', right_on='Filename', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    file               speaker      label Filename  \\\n",
      "0  0.wav         Alec Guinness      spoof    0.wav   \n",
      "1  1.wav         Alec Guinness      spoof    1.wav   \n",
      "2  2.wav          Barack Obama      spoof    2.wav   \n",
      "3  3.wav         Alec Guinness      spoof    3.wav   \n",
      "4  4.wav  Christopher Hitchens  bona-fide    4.wav   \n",
      "5  5.wav              Ayn Rand  bona-fide    5.wav   \n",
      "6  6.wav          Barack Obama      spoof    6.wav   \n",
      "7  7.wav          Donald Trump  bona-fide    7.wav   \n",
      "8  8.wav          Donald Trump  bona-fide    8.wav   \n",
      "9  9.wav         Alec Guinness  bona-fide    9.wav   \n",
      "\n",
      "                                               Audio  SampleRate  \n",
      "0  [0.0008559248, 5.8470447e-05, 0.0007754833, 0....       16000  \n",
      "1  [-0.00036084154, 0.000937727, -0.00047797145, ...       16000  \n",
      "2  [5.2883388e-05, 0.00010718728, 0.00014177158, ...       16000  \n",
      "3  [0.0036943506, 0.0015072429, -0.0018338299, -0...       16000  \n",
      "4  [-0.00015450451, -0.0002064928, 0.00040216927,...       16000  \n",
      "5  [0.0005254666, 0.0010142288, -0.0003861197, -0...       16000  \n",
      "6  [-0.00020708531, -0.00022848144, -0.0004157422...       16000  \n",
      "7  [-0.0007534244, -0.005679665, -0.0031570576, -...       16000  \n",
      "8  [-0.006333187, -0.0023277705, -0.001560311, -0...       16000  \n",
      "9  [-0.00026093068, -0.00013861844, -0.0003082217...       16000  \n"
     ]
    }
   ],
   "source": [
    "print(merged_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train (70%) and test (30%) sets\n",
    "train_data, test_data = train_test_split(merged_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the test data into test (15%) and evaluation (15%) sets\n",
    "test_data, eval_data = train_test_split(test_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = merged_data['label']\n",
    "\n",
    "# Convert MFCCs to a format suitable for machine learning\n",
    "X = [mfccs for filename, mfccs in audio_features]\n",
    "y = [1 if label == 'spoof' else 0 for label in labels]  # Convert labels to binary (1 for spoof, 0 for bona-fide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Determine the maximum length of MFCC feature vectors\n",
    "max_length = max(len(mfccs[0]) for mfccs in X_train)\n",
    "\n",
    "# Function to pad or truncate and reshape MFCCs\n",
    "def preprocess_mfccs(mfccs, max_length):\n",
    "    if len(mfccs[0]) < max_length:\n",
    "        padding = max_length - len(mfccs[0])\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, padding)), mode='constant')\n",
    "    elif len(mfccs[0]) > max_length:\n",
    "        mfccs = mfccs[:, :max_length]\n",
    "    return mfccs.reshape(-1)  # Reshape to a 1D array\n",
    "\n",
    "# Apply padding or truncation and reshape to all feature vectors\n",
    "X_train = [preprocess_mfccs(mfccs, max_length) for mfccs in X_train]\n",
    "X_test = [preprocess_mfccs(mfccs, max_length) for mfccs in X_test]\n",
    "X_eval = [preprocess_mfccs(mfccs, max_length) for mfccs in X_eval]\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_eval = scaler.transform(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6232431298510593\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-Score: 0.0\n",
      "Confusion Matrix:\n",
      "[[2971    0]\n",
      " [1796    0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.77      2971\n",
      "           1       0.00      0.00      0.00      1796\n",
      "\n",
      "    accuracy                           0.62      4767\n",
      "   macro avg       0.31      0.50      0.38      4767\n",
      "weighted avg       0.39      0.62      0.48      4767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautham/miniconda3/envs/tf-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gautham/miniconda3/envs/tf-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gautham/miniconda3/envs/tf-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/gautham/miniconda3/envs/tf-env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the evaluation set\n",
    "y_eval_pred = svm_model.predict(X_eval)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_eval, y_eval_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_eval, y_eval_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_eval, y_eval_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_eval, y_eval_pred)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_eval, y_eval_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(y_eval, y_eval_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.604992657856094\n",
      "Precision: 0.3753581661891118\n",
      "Recall: 0.07293986636971046\n",
      "F1-Score: 0.12214452214452215\n",
      "Confusion Matrix:\n",
      "[[2753  218]\n",
      " [1665  131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.93      0.75      2971\n",
      "           1       0.38      0.07      0.12      1796\n",
      "\n",
      "    accuracy                           0.60      4767\n",
      "   macro avg       0.50      0.50      0.43      4767\n",
      "weighted avg       0.53      0.60      0.51      4767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the evaluation set\n",
    "y_eval_pred = rf_model.predict(X_eval)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_eval, y_eval_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_eval, y_eval_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_eval, y_eval_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_eval, y_eval_pred)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_eval, y_eval_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(y_eval, y_eval_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
